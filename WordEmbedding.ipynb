{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding using python and Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading sample text from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file wiki_pages.txt in a list with a line per element\n",
    "with open('wiki_pages.txt') as f:\n",
    "    wiki_pages = f.readlines()\n",
    "\n",
    "for i in range(len(wiki_pages)):\n",
    "    wiki_pages[i] = wiki_pages[i].replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Italia\n",
      "Italia 1\n",
      "Nazionale di calcio dell'Italia\n",
      "Italia settentrionale\n",
      "Viaggio in Italia (film)\n",
      "Rai\n",
      "Regno d'Italia (1861-1946)\n",
      "Anniversario della liberazione d'Italia\n",
      "Italia meridionale\n",
      "ITA Airways\n",
      "Unione europea\n",
      "Stati membri dell'Unione europea\n",
      "Allargamento dell'Unione europea\n",
      "Targhe d'immatricolazione nell'Unione europea\n",
      "Comunità economica europea\n",
      "Consiglio dell'Unione europea\n",
      "Istituzioni dell'Unione europea\n",
      "Commissione europea\n",
      "Adesione della Turchia all'Unione europea\n",
      "Uscita del Regno Unito dall'Unione europea\n",
      "Diritti umani\n",
      "Dichiarazione universale dei diritti umani\n",
      "Corte europea dei diritti dell'uomo\n",
      "Dichiarazione dei diritti dell'uomo e del cittadino\n",
      "Corea del Nord\n",
      "Diritti umani in India\n",
      "Diritti umani negli Stati Uniti d'America\n",
      "Consiglio per i diritti umani delle Nazioni Unite\n",
      "Tre generazioni dei diritti umani\n",
      "Diritti umani in Bangladesh\n",
      "Bruxelles\n",
      "Bruxelles (comune)\n",
      "Regione di Bruxelles-Capitale\n",
      "Grand-Place (Bruxelles)\n",
      "Belgio\n",
      "Dio esiste e vive a Bruxelles\n",
      "Aeroporto di Bruxelles-National\n",
      "Griffone di Bruxelles\n",
      "Cavolo di Bruxelles\n",
      "Université libre de Bruxelles\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    chars_to_remove = ['\\n', '\\t', '\\r', ',', '.', '!']\n",
    "    for c in chars_to_remove:\n",
    "        text = text.replace(c, ' ')\n",
    "    return text\n",
    "\n",
    "wikipedia.set_lang(\"it\")\n",
    "train_text = []\t\n",
    "for page in wiki_pages:\n",
    "    all_articles = wikipedia.search(page)\n",
    "    \n",
    "    for article in all_articles:\n",
    "        try:\n",
    "            article = wikipedia.page(article)\n",
    "            train_text.append(clean_text(article.content))\n",
    "            print(article.title)\n",
    "        except:\n",
    "            print(\"Page not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words = 27509\n"
     ]
    }
   ],
   "source": [
    "# Create a list of unique words in the text\n",
    "words = []\n",
    "for text in train_text:\n",
    "    words += text.split()\n",
    "unique_words = list(set(words))\n",
    "unique_words_count = len(unique_words)\n",
    "print(\"Unique words =\", unique_words_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_words_index = {unique_words[i]:i for i in range(unique_words_count)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creation of data to fit\n",
    "\n",
    "The input of the neural network is a vector of $1$ (only in the position of the current word) and $0$ (in all the other positions). The output of the network should be the next word in the text (encoded as the same type of vector with $0$ and $1$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.zeros((len(words), unique_words_count), dtype=bool)\n",
    "y = np.zeros((len(words), unique_words_count), dtype=bool)\n",
    "for i in range(len(words)-1):\n",
    "    x[i, unique_words_index[words[i]]] = 1\n",
    "    y[i, unique_words_index[words[i+1]]] = 1\n",
    "    \n",
    "# Split into train and test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create deep neural network\n",
    "\n",
    "Each input neuron correspond to 1 word and the number of neuron in the first hidden layer corresponds to the number of dimension of the word embedding. After that we have another fully connected layer that pass through a softmax to output the next word probability. This network is trained to predict the next word given a single word in a sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, 27509)]           0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 2)                 55020     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 27509)             82527     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 137547 (537.29 KB)\n",
      "Trainable params: 137547 (537.29 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Softmax\n",
    "from keras.models import Model\n",
    "\n",
    "def create_network(input_neurons, word_embedding_dimension):\n",
    "    xin = Input(shape=(input_neurons,))\n",
    "    x = Dense(word_embedding_dimension, activation='relu')(xin)\n",
    "    x = Dense(input_neurons, activation='softmax')(x)\n",
    "    \n",
    "    return Model(inputs=xin,outputs=x)\n",
    "\n",
    "WORD_EMBEDDING_DIMENSION = 2\n",
    "\n",
    "model = create_network(unique_words_count, WORD_EMBEDDING_DIMENSION)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 8.4181 - accuracy: 0.0365 - val_loss: 8.1517 - val_accuracy: 0.0363\n",
      "Epoch 2/100\n",
      "4309/4309 [==============================] - 33s 8ms/step - loss: 7.8347 - accuracy: 0.0367 - val_loss: 8.2939 - val_accuracy: 0.0363\n",
      "Epoch 3/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 7.7604 - accuracy: 0.0367 - val_loss: 8.4455 - val_accuracy: 0.0362\n",
      "Epoch 4/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 7.7172 - accuracy: 0.0366 - val_loss: 8.5282 - val_accuracy: 0.0362\n",
      "Epoch 5/100\n",
      "4309/4309 [==============================] - 37s 8ms/step - loss: 7.6755 - accuracy: 0.0367 - val_loss: 8.5528 - val_accuracy: 0.0370\n",
      "Epoch 6/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 7.6207 - accuracy: 0.0363 - val_loss: 8.5514 - val_accuracy: 0.0368\n",
      "Epoch 7/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.5565 - accuracy: 0.0363 - val_loss: 8.5352 - val_accuracy: 0.0378\n",
      "Epoch 8/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 7.4936 - accuracy: 0.0376 - val_loss: 8.5340 - val_accuracy: 0.0398\n",
      "Epoch 9/100\n",
      "4309/4309 [==============================] - 34s 8ms/step - loss: 7.4383 - accuracy: 0.0379 - val_loss: 8.5140 - val_accuracy: 0.0398\n",
      "Epoch 10/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 7.3908 - accuracy: 0.0380 - val_loss: 8.5164 - val_accuracy: 0.0405\n",
      "Epoch 11/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.3485 - accuracy: 0.0387 - val_loss: 8.5200 - val_accuracy: 0.0412\n",
      "Epoch 12/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.3100 - accuracy: 0.0398 - val_loss: 8.5173 - val_accuracy: 0.0424\n",
      "Epoch 13/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.2757 - accuracy: 0.0420 - val_loss: 8.5131 - val_accuracy: 0.0436\n",
      "Epoch 14/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 7.2445 - accuracy: 0.0424 - val_loss: 8.5125 - val_accuracy: 0.0447\n",
      "Epoch 15/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 7.2157 - accuracy: 0.0434 - val_loss: 8.5324 - val_accuracy: 0.0448\n",
      "Epoch 16/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 7.1893 - accuracy: 0.0446 - val_loss: 8.5269 - val_accuracy: 0.0450\n",
      "Epoch 17/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 7.1642 - accuracy: 0.0466 - val_loss: 8.5412 - val_accuracy: 0.0492\n",
      "Epoch 18/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 7.1406 - accuracy: 0.0490 - val_loss: 8.5474 - val_accuracy: 0.0510\n",
      "Epoch 19/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.1183 - accuracy: 0.0501 - val_loss: 8.5537 - val_accuracy: 0.0498\n",
      "Epoch 20/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 7.0970 - accuracy: 0.0509 - val_loss: 8.5650 - val_accuracy: 0.0517\n",
      "Epoch 21/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 7.0770 - accuracy: 0.0523 - val_loss: 8.5763 - val_accuracy: 0.0507\n",
      "Epoch 22/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.0580 - accuracy: 0.0534 - val_loss: 8.5778 - val_accuracy: 0.0525\n",
      "Epoch 23/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 7.0400 - accuracy: 0.0554 - val_loss: 8.5940 - val_accuracy: 0.0530\n",
      "Epoch 24/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 7.0231 - accuracy: 0.0573 - val_loss: 8.5979 - val_accuracy: 0.0555\n",
      "Epoch 25/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 7.0072 - accuracy: 0.0584 - val_loss: 8.6070 - val_accuracy: 0.0553\n",
      "Epoch 26/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.9922 - accuracy: 0.0596 - val_loss: 8.6187 - val_accuracy: 0.0555\n",
      "Epoch 27/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.9779 - accuracy: 0.0595 - val_loss: 8.6281 - val_accuracy: 0.0554\n",
      "Epoch 28/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.9647 - accuracy: 0.0594 - val_loss: 8.6394 - val_accuracy: 0.0569\n",
      "Epoch 29/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.9521 - accuracy: 0.0603 - val_loss: 8.6564 - val_accuracy: 0.0562\n",
      "Epoch 30/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.9401 - accuracy: 0.0601 - val_loss: 8.6637 - val_accuracy: 0.0565\n",
      "Epoch 31/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.9289 - accuracy: 0.0605 - val_loss: 8.6694 - val_accuracy: 0.0551\n",
      "Epoch 32/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.9183 - accuracy: 0.0609 - val_loss: 8.6788 - val_accuracy: 0.0566\n",
      "Epoch 33/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.9083 - accuracy: 0.0613 - val_loss: 8.6887 - val_accuracy: 0.0569\n",
      "Epoch 34/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.8985 - accuracy: 0.0623 - val_loss: 8.6977 - val_accuracy: 0.0572\n",
      "Epoch 35/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.8893 - accuracy: 0.0622 - val_loss: 8.7067 - val_accuracy: 0.0547\n",
      "Epoch 36/100\n",
      "4309/4309 [==============================] - 34s 8ms/step - loss: 6.8810 - accuracy: 0.0627 - val_loss: 8.7188 - val_accuracy: 0.0575\n",
      "Epoch 37/100\n",
      "4309/4309 [==============================] - 34s 8ms/step - loss: 6.8727 - accuracy: 0.0633 - val_loss: 8.7285 - val_accuracy: 0.0579\n",
      "Epoch 38/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.8648 - accuracy: 0.0637 - val_loss: 8.7360 - val_accuracy: 0.0568\n",
      "Epoch 39/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.8571 - accuracy: 0.0633 - val_loss: 8.7462 - val_accuracy: 0.0571\n",
      "Epoch 40/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.8501 - accuracy: 0.0641 - val_loss: 8.7539 - val_accuracy: 0.0580\n",
      "Epoch 41/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.8430 - accuracy: 0.0650 - val_loss: 8.7653 - val_accuracy: 0.0590\n",
      "Epoch 42/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.8364 - accuracy: 0.0651 - val_loss: 8.7782 - val_accuracy: 0.0583\n",
      "Epoch 43/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.8300 - accuracy: 0.0654 - val_loss: 8.7781 - val_accuracy: 0.0591\n",
      "Epoch 44/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.8237 - accuracy: 0.0656 - val_loss: 8.7889 - val_accuracy: 0.0587\n",
      "Epoch 45/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.8175 - accuracy: 0.0678 - val_loss: 8.8004 - val_accuracy: 0.0575\n",
      "Epoch 46/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.8116 - accuracy: 0.0678 - val_loss: 8.8044 - val_accuracy: 0.0604\n",
      "Epoch 47/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.8060 - accuracy: 0.0704 - val_loss: 8.8116 - val_accuracy: 0.0565\n",
      "Epoch 48/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.8003 - accuracy: 0.0715 - val_loss: 8.8184 - val_accuracy: 0.0586\n",
      "Epoch 49/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7951 - accuracy: 0.0726 - val_loss: 8.8265 - val_accuracy: 0.0585\n",
      "Epoch 50/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7899 - accuracy: 0.0729 - val_loss: 8.8334 - val_accuracy: 0.0595\n",
      "Epoch 51/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.7851 - accuracy: 0.0734 - val_loss: 8.8375 - val_accuracy: 0.0565\n",
      "Epoch 52/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.7803 - accuracy: 0.0736 - val_loss: 8.8483 - val_accuracy: 0.0579\n",
      "Epoch 53/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7758 - accuracy: 0.0731 - val_loss: 8.8616 - val_accuracy: 0.0580\n",
      "Epoch 54/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.7715 - accuracy: 0.0739 - val_loss: 8.8625 - val_accuracy: 0.0577\n",
      "Epoch 55/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.7672 - accuracy: 0.0731 - val_loss: 8.8829 - val_accuracy: 0.0569\n",
      "Epoch 56/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.7633 - accuracy: 0.0736 - val_loss: 8.8808 - val_accuracy: 0.0578\n",
      "Epoch 57/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7590 - accuracy: 0.0738 - val_loss: 8.8873 - val_accuracy: 0.0589\n",
      "Epoch 58/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7549 - accuracy: 0.0738 - val_loss: 8.8972 - val_accuracy: 0.0581\n",
      "Epoch 59/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7510 - accuracy: 0.0743 - val_loss: 8.9064 - val_accuracy: 0.0574\n",
      "Epoch 60/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7473 - accuracy: 0.0747 - val_loss: 8.9079 - val_accuracy: 0.0581\n",
      "Epoch 61/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.7437 - accuracy: 0.0743 - val_loss: 8.9143 - val_accuracy: 0.0581\n",
      "Epoch 62/100\n",
      "4309/4309 [==============================] - 37s 8ms/step - loss: 6.7403 - accuracy: 0.0746 - val_loss: 8.9359 - val_accuracy: 0.0575\n",
      "Epoch 63/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7367 - accuracy: 0.0744 - val_loss: 8.9411 - val_accuracy: 0.0588\n",
      "Epoch 64/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7335 - accuracy: 0.0742 - val_loss: 8.9497 - val_accuracy: 0.0569\n",
      "Epoch 65/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.7301 - accuracy: 0.0745 - val_loss: 8.9550 - val_accuracy: 0.0590\n",
      "Epoch 66/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.7271 - accuracy: 0.0747 - val_loss: 8.9641 - val_accuracy: 0.0585\n",
      "Epoch 67/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.7237 - accuracy: 0.0755 - val_loss: 8.9683 - val_accuracy: 0.0576\n",
      "Epoch 68/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.7207 - accuracy: 0.0758 - val_loss: 8.9777 - val_accuracy: 0.0576\n",
      "Epoch 69/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7179 - accuracy: 0.0760 - val_loss: 8.9835 - val_accuracy: 0.0573\n",
      "Epoch 70/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7151 - accuracy: 0.0765 - val_loss: 8.9902 - val_accuracy: 0.0586\n",
      "Epoch 71/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7122 - accuracy: 0.0767 - val_loss: 8.9948 - val_accuracy: 0.0590\n",
      "Epoch 72/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.7094 - accuracy: 0.0769 - val_loss: 9.0102 - val_accuracy: 0.0567\n",
      "Epoch 73/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.7066 - accuracy: 0.0777 - val_loss: 9.0256 - val_accuracy: 0.0588\n",
      "Epoch 74/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7040 - accuracy: 0.0775 - val_loss: 9.0132 - val_accuracy: 0.0569\n",
      "Epoch 75/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.7014 - accuracy: 0.0776 - val_loss: 9.0306 - val_accuracy: 0.0583\n",
      "Epoch 76/100\n",
      "4309/4309 [==============================] - 35s 8ms/step - loss: 6.6988 - accuracy: 0.0775 - val_loss: 9.0374 - val_accuracy: 0.0574\n",
      "Epoch 77/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6966 - accuracy: 0.0778 - val_loss: 9.0455 - val_accuracy: 0.0584\n",
      "Epoch 78/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6938 - accuracy: 0.0774 - val_loss: 9.0558 - val_accuracy: 0.0579\n",
      "Epoch 79/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.6916 - accuracy: 0.0776 - val_loss: 9.0623 - val_accuracy: 0.0592\n",
      "Epoch 80/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.6893 - accuracy: 0.0779 - val_loss: 9.0719 - val_accuracy: 0.0591\n",
      "Epoch 81/100\n",
      "4309/4309 [==============================] - 38s 9ms/step - loss: 6.6869 - accuracy: 0.0780 - val_loss: 9.0833 - val_accuracy: 0.0582\n",
      "Epoch 82/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.6846 - accuracy: 0.0784 - val_loss: 9.0898 - val_accuracy: 0.0589\n",
      "Epoch 83/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6825 - accuracy: 0.0783 - val_loss: 9.0949 - val_accuracy: 0.0578\n",
      "Epoch 84/100\n",
      "4309/4309 [==============================] - 37s 8ms/step - loss: 6.6801 - accuracy: 0.0780 - val_loss: 9.1032 - val_accuracy: 0.0578\n",
      "Epoch 85/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6779 - accuracy: 0.0780 - val_loss: 9.1182 - val_accuracy: 0.0592\n",
      "Epoch 86/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6759 - accuracy: 0.0784 - val_loss: 9.1183 - val_accuracy: 0.0590\n",
      "Epoch 87/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.6737 - accuracy: 0.0782 - val_loss: 9.1250 - val_accuracy: 0.0598\n",
      "Epoch 88/100\n",
      "4309/4309 [==============================] - 42s 10ms/step - loss: 6.6718 - accuracy: 0.0787 - val_loss: 9.1347 - val_accuracy: 0.0596\n",
      "Epoch 89/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6698 - accuracy: 0.0785 - val_loss: 9.1387 - val_accuracy: 0.0593\n",
      "Epoch 90/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6677 - accuracy: 0.0786 - val_loss: 9.1589 - val_accuracy: 0.0590\n",
      "Epoch 91/100\n",
      "4309/4309 [==============================] - 43s 10ms/step - loss: 6.6658 - accuracy: 0.0786 - val_loss: 9.1687 - val_accuracy: 0.0587\n",
      "Epoch 92/100\n",
      "4309/4309 [==============================] - 40s 9ms/step - loss: 6.6640 - accuracy: 0.0789 - val_loss: 9.1695 - val_accuracy: 0.0594\n",
      "Epoch 93/100\n",
      "4309/4309 [==============================] - 41s 10ms/step - loss: 6.6621 - accuracy: 0.0790 - val_loss: 9.1817 - val_accuracy: 0.0597\n",
      "Epoch 94/100\n",
      "4309/4309 [==============================] - 43s 10ms/step - loss: 6.6601 - accuracy: 0.0790 - val_loss: 9.1817 - val_accuracy: 0.0591\n",
      "Epoch 95/100\n",
      "4309/4309 [==============================] - 42s 10ms/step - loss: 6.6585 - accuracy: 0.0793 - val_loss: 9.1901 - val_accuracy: 0.0586\n",
      "Epoch 96/100\n",
      "4309/4309 [==============================] - 66s 15ms/step - loss: 6.6567 - accuracy: 0.0793 - val_loss: 9.2013 - val_accuracy: 0.0602\n",
      "Epoch 97/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.6551 - accuracy: 0.0790 - val_loss: 9.2133 - val_accuracy: 0.0600\n",
      "Epoch 98/100\n",
      "4309/4309 [==============================] - 37s 9ms/step - loss: 6.6530 - accuracy: 0.0795 - val_loss: 9.2226 - val_accuracy: 0.0595\n",
      "Epoch 99/100\n",
      "4309/4309 [==============================] - 39s 9ms/step - loss: 6.6515 - accuracy: 0.0793 - val_loss: 9.2298 - val_accuracy: 0.0599\n",
      "Epoch 100/100\n",
      "4309/4309 [==============================] - 36s 8ms/step - loss: 6.6499 - accuracy: 0.0797 - val_loss: 9.2321 - val_accuracy: 0.0604\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1ad397e8100>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, shuffle=True, epochs=100, batch_size=32,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luca\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('word_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_words[np.argmax(x_test[0:1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_word(model, input_word):\n",
    "    input_vector = np.zeros((1, unique_words_count))\n",
    "    input_vector[0, unique_words_index[input_word]] = 1\n",
    "    return unique_words[np.argmax(model.predict(input_vector))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_word(model, 'di')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the word embedding from the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27509, 2)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The embedding of each word is defined by the weights of the first layer\n",
    "word_embedding = model.layers[1].get_weights()[0]\n",
    "word_embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.70760226, -0.5365534 ], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding[unique_words_index['di']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.23127148, -0.02789657], dtype=float32)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding[unique_words_index['e']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_closest_words(words_embeddings, word, n=5):\n",
    "    word_index = unique_words_index[word]\n",
    "    word_embedding = words_embeddings[word_index]\n",
    "    distances = np.linalg.norm(words_embeddings - word_embedding, axis=1)\n",
    "    closest_words = [unique_words[i] for i in np.argsort(distances)[1:n+1]]\n",
    "    return closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['narrativa', 'rivelano', 'contrario', 'RaiPlay', 'decenni']"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_closest_words(word_embedding, 'Roma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2000', 'documento', 'Rice', 'ricorrenza', 'Clarisse']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = word_embedding[unique_words_index['Italia']] - word_embedding[unique_words_index['Roma']] + word_embedding[unique_words_index['Parigi']]\n",
    "distances = np.linalg.norm(word_embedding - test, axis=1)\n",
    "closest_words = [unique_words[i] for i in np.argsort(distances)[0:5]]\n",
    "closest_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
